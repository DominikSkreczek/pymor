# This file is part of the pyMOR project (https://www.pymor.org).
# Copyright pyMOR developers and contributors. All rights reserved.
# License: BSD 2-Clause License (https://opensource.org/licenses/BSD-2-Clause)

from copy import deepcopy

import numpy as np

from pymor.algorithms.bfgs import error_aware_bfgs
from pymor.core.base import BasicObject, abstractmethod
from pymor.core.defaults import defaults
from pymor.core.exceptions import ExtensionError, TRError
from pymor.core.logger import getLogger
from pymor.parameters.base import Mu


@defaults('beta', 'radius', 'shrink_factor', 'miniter', 'maxiter', 'miniter_subproblem', 'maxiter_subproblem',
          'tol_criticality', 'radius_tol', 'rtol_output', 'rtol_mu', 'tol_sub', 'stagnation_window',
          'stagnation_threshold')
def trust_region(surrogate, parameter_space=None, initial_guess=None, beta=.95, radius=.1,
                 shrink_factor=.5, miniter=0, maxiter=30, miniter_subproblem=0, maxiter_subproblem=400,
                 tol_criticality=1e-6, radius_tol=.75, rtol_output=1e-16, rtol_mu=1e-16, tol_sub=1e-8,
                 armijo_alpha=1e-4, line_search_params=None, stagnation_window=3, stagnation_threshold=np.inf):
    """Error-aware trust region algorithm.

    This method solves the optimization problem ::

        min J(mu), mu in C

    for a model with an output functional :math:`J` depending on a box-constrained `mu` using
    an adaptive trust region method.

    The main idea for the algorithm can be found in :cite:`YM13`, and an application to
    box-constrained parameters with possible enlarging of the trust radius in :cite:`KMOSV21`.

    This method contrasts itself from :func:`scipy.optimize.minimize` in the computation of the
    trust region: `scipy` TR implementations use a metric distance, whereas this function uses an
    error estimator obtained from the surrogate. Additionally, the cheap model function
    surrogate here is only updated for each outer iteration, not entirely reconstructed.

    Parameters
    ----------
    surrogate
        The :class:`TRSurrogate` used to generate the surrogate model and estimate the output error.
    parameter_space
        If not `None`, the |ParameterSpace| for enforcing the box constraints on the
        |parameter values| `mu`. Otherwise a |ParameterSpace| with lower bound -1
        and upper bound 1 is used.
    initial_guess
        If not `None`, |parameter values| containing an initial guess for the
        solution `mu`. Otherwise, random |parameter values| from the `parameter_space` are
        chosen as the initial value.
    beta
        The factor used to check if the current |parameter values| are close to the
        trust region boundary.
    radius
        The radius of the initial trust region.
    shrink_factor
        The factor by which the trust region is shrunk. If the trust region radius is increased,
        it is increased by `1. / shrink_factor`.
    miniter
        Minimum amount of iterations to perform.
    maxiter
        Fail if the iteration count reaches this value without converging.
    miniter_subproblem
        Minimum amount of iterations to perform in the BFGS subproblem.
    maxiter_subproblem
        Fail the BFGS subproblem if the iteration count reaches this value without converging.
    tol_criticality
        Finish when the current |parameter values| fulfill the approximate first order critical
        optimality condition with a value below this threshold.
    radius_tol
        Threshold for increasing the trust region radius upon extending the reduced order model.
    rtol_output
        See `rtol_output` of :func:`~pymor.algorithms.bfgs.error_aware_bfgs`.
    rtol_mu
        See `rtol_mu` of :func:`~pymor.algorithms.bfgs.error_aware_bfgs`.
    tol_sub
        See `tol_sub` of :func:`~pymor.algorithms.bfgs.error_aware_bfgs`.
    armijo_alpha
        Threshold for the constrained Armijo condition.
        See :func:`~pymor.algorithms.line_search.constrained_armijo`.
    line_search_params
        Dictionary of additional parameters passed to the Armijo line search method.
    stagnation_window
        Finish when the parameter update has not been enlarged by a factor of
        `stagnation_threshold` during the last `stagnation_window` iterations.
    stagnation_threshold
        See `stagnation_window`.

    Returns
    -------
    mu
        |Numpy array| containing the computed optimal |parameter values|.
    data
        Dict containing the following fields:

            :mus:             `list` of the current solution after each iteration.
            :mu_norms:        |NumPy array| of the solution norms after each iteration.
            :subproblem_data: `list` of data generated by the individual subproblems.

    Raises
    ------
    TRError
        Raised if the TR algorithm failed to converge.
    """
    assert shrink_factor > 0.

    logger = getLogger('pymor.algorithms.tr.trust_region')
    logger.info(f'Started error-aware adaptive TR algorithm for {surrogate.fom.output_functional}.')

    reductor = surrogate.reductor

    if parameter_space is None:
        logger.warn('No parameter space given. Assuming uniform parameter bounds of (-1, 1).')
        parameter_space = reductor.fom.parameters.space(-1., 1.)

    if initial_guess is None:
        initial_guess = parameter_space.sample_randomly(1)[0]
        mu = initial_guess.to_numpy()
    else:
        mu = initial_guess.to_numpy() if isinstance(initial_guess, Mu) else initial_guess

    def error_aware_bfgs_criterion(new_mu, current_value):
        output_error = surrogate.estimate_output_error(new_mu)
        return output_error / abs(current_value) >= beta * radius

    def error_aware_line_search_criterion(starting_point, initial_value, current_value, step,
                                          line_search_beta, direction, slope):
        initial_mu = starting_point
        new_mu = initial_mu + step * direction

        # check if the new parameter is outside of the parameter space's bounds
        if not np.allclose(new_mu - parameter_space.clip(new_mu).to_numpy(), 0.):
            return False

        # check the convergence conditions of the line search
        # the first condition corresponds to the usual armijo descent criterion
        # the second condition checks if the new relative error is still within the trust region
        output_error = surrogate.estimate_output_error(new_mu)
        return (current_value <= initial_value - (armijo_alpha / step) * np.linalg.norm(new_mu - initial_mu)**2
            and output_error / abs(current_value) <= radius)

    data = {'subproblem_data': []}

    # compute norms
    mu_norm = np.linalg.norm(mu)
    update_norms = []
    foc_norms = []
    data['mus'] = [mu.copy()]

    old_rom_output = surrogate.output(mu)
    old_fom_output = surrogate.fom.output(mu)

    first_order_criticality = np.inf
    iteration = 0
    while True:
        with logger.block(f'Starting adaptive TR algorithm iteration {iteration + 1} with radius {radius}...'):
            rejected = False

            if iteration >= miniter:
                if first_order_criticality < tol_criticality:
                    logger.info(
                        f'TR converged in {iteration} iterations because first order criticality tolerance of' \
                        f' {tol_criticality} was reached. The reduced basis is of size {surrogate.rb_size()}.')
                    break
                if iteration >= maxiter:
                    logger.info(f'Maximum iterations reached. Failed to converge after {iteration} iterations.')
                    raise TRError('Failed to converge after the maximum amount of iterations.')

            iteration += 1

            # solve the subproblem using bfgs
            old_mu = mu.copy()

            with logger.block(f'Solving subproblem for mu {mu} with BFGS...'):
                mu, sub_data = error_aware_bfgs(
                    surrogate.rom, parameter_space, initial_guess=mu, miniter=miniter_subproblem,
                    maxiter=maxiter_subproblem, rtol_output=rtol_output, rtol_mu=rtol_mu, tol_sub=tol_sub,
                    line_search_params=line_search_params, stagnation_window=stagnation_window,
                    stagnation_threshold=stagnation_threshold, error_aware=True,
                    error_criterion=error_aware_bfgs_criterion,
                    line_search_error_criterion=error_aware_line_search_criterion)

            # first BFGS iterate is AGC point
            index = 1 if len(sub_data['mus']) > 1 else 0
            compare_output = surrogate.output(sub_data['mus'][index])
            estimate_output = surrogate.estimate_output_error(mu)
            current_output = surrogate.output(mu)

            with logger.block('Running output checks for TR parameters.'):
                if current_output + estimate_output < compare_output:
                    surrogate.extend(mu)
                    current_fom_output = surrogate.fom.output(mu)
                    fom_output_diff = old_fom_output - current_fom_output
                    rom_output_diff = old_rom_output - current_output
                    if fom_output_diff >= radius_tol * rom_output_diff:
                        # increase the radius if the model confidence is high enough
                        # model confidence is deemed high if a small decrease in the ROM's output
                        # results in a larger decrease of the FOM's output
                        radius /= shrink_factor

                    msg = 'Estimated output smaller than previous output.'
                elif current_output - estimate_output > compare_output:
                    # reject new mu
                    rejected = True
                    # shrink the radius
                    radius *= shrink_factor

                    msg = 'Estimated output larger than previous output.'
                else:
                    surrogate.extend(mu)
                    current_output = surrogate.new_output(mu)
                    if current_output <= compare_output:
                        current_fom_output = surrogate.fom.output(mu)
                        fom_output_diff = old_fom_output - current_fom_output
                        rom_output_diff = old_rom_output - current_output
                        if fom_output_diff >= radius_tol * rom_output_diff:
                            # increase the radius if the model confidence is high enough
                            radius /= shrink_factor

                        msg = 'Updated model output smaller than previous output.'
                    else:
                        # reject new mu
                        rejected = True
                        # shrink the radius
                        radius *= shrink_factor

                        msg = 'Updated model output larger than previous output.'

            # handle parameter rejection
            if not rejected:
                data['mus'].append(mu.copy())
                mu_norm = np.linalg.norm(mu)
                update_norms.append(np.linalg.norm(mu - old_mu))

                data['subproblem_data'].append(sub_data)

                with logger.block('Computing first order criticality...'):
                    gradient = surrogate.rom.parameters.parse(surrogate.fom.output_d_mu(mu)).to_numpy()
                    first_order_criticality = np.linalg.norm(mu - parameter_space.clip(mu - gradient).to_numpy())
                    foc_norms.append(first_order_criticality)

                surrogate.accept()
                logger.info(f'Current mu iterate accepted: {msg}')

                old_rom_output = current_output
            else:
                mu = old_mu
                surrogate.reject()
                logger.info(f'Current mu iterate rejected: {msg}')

            if not np.isfinite(mu_norm):
                raise TRError('Failed to converge.')

    logger.info('')

    data['update_norms'] = np.array(update_norms)
    data['foc_norms'] = np.array(foc_norms)
    data['iterations'] = iteration
    data['fom_evaluations'] = surrogate.fom_evaluations
    data['rom_evaluations'] = surrogate.rom_evaluations
    data['enrichments'] = surrogate.enrichments

    return mu, data


class TRSurrogate(BasicObject):
    """Base class for :func:`trust_region` surrogates.

    Not intended to be used directly.
    """

    def __init__(self, reductor, initial_guess):
        self.__auto_init(locals())

        self.fom_evaluations = 0
        self.rom_evaluations = 0
        self.enrichments = 0

        # generate a first rom if none was given
        self._fom = reductor.fom
        if isinstance(initial_guess, Mu):
            initial_guess = initial_guess.to_numpy()
        assert isinstance(initial_guess, np.ndarray)
        assert self._fom.parameters.assert_compatible(self._fom.parameters.parse(initial_guess))
        self.extend(initial_guess)
        self.accept()

        assert self._fom.dim_output == 1
        assert self.rom.dim_output == 1

        # initialize placeholders for extension
        self.new_reductor = None
        self.new_rom = None

    @property
    def fom(self):
        self.fom_evaluations += 1
        return self._fom

    def output(self, mu):
        self.rom_evaluations += 1
        return self.rom.output(mu)[0, 0]

    @abstractmethod
    def estimate_output_error(self, mu):
        pass

    def extend(self, mu):
        """Extend the current ROM for new |parameter values|.

        Parameters
        ----------
        mu
            The `Mu` instance for which an extension is computed.
        """
        with self.logger.block('Extending the basis...'):
            U_h_mu = self.fom.solve(mu)
            self.new_reductor = deepcopy(self.reductor)
            try:
                self.new_reductor.extend_basis(U_h_mu)
            except ExtensionError:
                self.new_reductor = self.reductor
            self.new_rom = self.new_reductor.reduce()

    def new_output(self, mu):
        assert self.new_rom is not None, 'No new ROM found. Did you forget to call surrogate.extend()?'
        assert self.new_rom.dim_output == 1
        self.rom_evaluations += 1
        return self.new_rom.output(mu)[0, 0]

    def accept(self):
        """Accept the new ROM.

        This function is intended to be called after :meth:`~TRSurrogate.extend` was called.
        """
        assert self.new_rom is not None, 'No new ROM found. Did you forget to call surrogate.extend()?'
        self.rom = self.new_rom
        self.reductor = self.new_reductor
        self.new_rom = None
        self.new_reductor = None
        self.enrichments += 1

    def reject(self):
        """Reject the new ROM.

        This function is intended to be called after :func:`extend` was called.
        """
        self.new_rom = None
        self.new_reductor = None

    def rb_size(self):
        return len(self.reductor.bases['RB'])


class SimpleTRSurrogate(TRSurrogate):
    """Surrogate for the :func:`trust_region` using a simple error estimator.

    Parameters
    ----------
    reductor
        The reductor used to generate the reduced order models and estimate the output error.
    initial_guess
        The |parameter values| containing an initial guess for the optimal parameter value.
    """

    def estimate_output_error(self, mu):
        U_mu, pr_err = self.rom.solve(mu, return_error_estimate=True)
        return pr_err * (2 * self.rom.l2_norm(U_mu) + pr_err)


class DualTRSurrogate(TRSurrogate):
    """Surrogate for the :func:`trust_region` using a simple dual error estimator.

    Parameters
    ----------
    reductor
        The reductor used to generate the reduced order models and estimate the output error.
    initial_guess
        The |parameter values| containing an initial guess for the optimal parameter value.
    """

    def extend(self, mu):
        """Extend the current ROM for new |parameter values| with primal and dual solutions.

        Parameters
        ----------
        mu
            The `Mu` instance for which an extension is computed.
        """
        with self.logger.block('Extending the basis with primal and dual...'):
            U_h_mu = self.fom.solve(mu)
            jacobian = self._fom.output_functional.jacobian(U_h_mu, self._fom.parameters.parse(mu))
            dual_solutions = self._fom.solution_space.empty()
            for d in range(self._fom.dim_output):
                dual_problem = self.fom.with_(operator=self._fom.operator.H, rhs=jacobian.H.as_range_array(mu)[d])
                P_h_mu = dual_problem.solve(mu)
                dual_solutions.append(P_h_mu)

            self.new_reductor = deepcopy(self.reductor)
            try:
                self.new_reductor.extend_basis(U_h_mu)
            except ExtensionError:
                self.new_reductor = self.reductor
            # it can happen that primal is successful but duals are not.
            try:
                self.new_reductor.extend_basis(dual_solutions)
            except ExtensionError:
                pass
            self.new_rom = self.new_reductor.reduce()

    def estimate_output_error(self, mu):
        U_mu, pr_err = self.rom.solve(mu, return_error_estimate=True)

        jacobian = self.rom.output_functional.jacobian(U_mu, self.rom.parameters.parse(mu))
        dual_sols = self.rom.solution_space.empty()
        dual_terms = self._fom.solution_space.empty()
        for d in range(self.rom.dim_output):
            dual_problem = self.rom.with_(operator=self.rom.operator.H, rhs=jacobian.H.as_range_array(mu)[d])
            dual_sol = dual_problem.solve(mu)
            dual_sols.append(dual_sol)
            op_H = self.fom.operator.H.apply(self.reductor.reconstruct(dual_sol), self._fom.parameters.parse(mu))
            dual_terms.append(op_H)
        jac_h = self.fom.output_functional.jacobian(self.reductor.reconstruct(U_mu), self._fom.parameters.parse(mu))
        j_h = jac_h.as_vector(mu)
        res_terms = dual_terms - j_h
        riesz = self.reductor.products['RB'].apply_inverse(res_terms)
        # Note: these are also fom_evaluations
        dual_estimate = np.sqrt(self.reductor.products['RB'].apply2(riesz, riesz))
        errs = (dual_estimate + pr_err) * pr_err
        return errs
